_target_: lfq_bet.models.lfq_bet.LFQBeT

# optimizer:
#   _target_: torch.optim.Adam
#   _partial_: true
#   lr: 0.001
#   weight_decay: 0.0

# scheduler:
#   _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
#   _partial_: true
#   mode: min
#   factor: 0.1
#   patience: 10

optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 5e-5
  betas: [0.9, 0.999]

scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingWarmRestarts
  _partial_: true
  T_0: 1000


gpt_cfg:
  _target_: lfq_bet.models.nets.gpt.GPTConfig
  block_size: 110
  # input_dim: ${env.obs_dim}
#TODO:vocab should be read the codebook size
  vocab_size: 4096
  n_layer: 6
  n_head: 6
  n_embd: 120
  bias: false

act_tok_lr: 0.001
state_tok_lr: null


# compile model for faster training with pytorch 2.0
compile: true
